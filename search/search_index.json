{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SHEET / MOS-Bench","text":"<p>MOS-Bench is a benchmark designed to benchmark the generalization abilities of subjective speech quality assessment (SSQA) models. SHEET stands for the Speech Human Evaluation Estimation Toolkit. SHEET was designed to conduct research experiments with MOS-Bench.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>MOS-Bench is the first large-scale collection of training and testing datasets for SSQA, covering a wide range of domains, including synthetic speech from text-to-speech (TTS), voice conversion (VC), singing voice synthetis (SVS) systems, and distorted speech with artificial and real noise, clipping, transmission, reverb, etc. Researchers can use the testing sets to benchmark their SSQA model.</li> <li>This repository aims to provide training recipes. While there are many off-the-shelf speech quality evaluators like DNSMOS, SpeechMOS and speechmetrics, most of them do not provide training recipes, thus are not research-oriented. Newcomers may utilize this repo as a starting point to SSQA research.</li> </ul>"},{"location":"#mos-bench","title":"MOS-Bench","text":"<p>As of September 2025, MOS-Bench has 8 training sets and 17 test sets. See the MOS-Bench page for more.</p>"},{"location":"#instsallation","title":"Instsallation","text":""},{"location":"#editable-installation-with-virtualenv","title":"Editable installation with virtualenv","text":"<p>You don't need to prepare an environment (using conda, etc.) first. The following commands will automatically construct a virtual environment in <code>tools/</code>. When you run the recipes, the scripts will automatically activate the virtual environment.</p> <pre><code>git clone https://github.com/unilight/sheet.git\ncd sheet/tools\nmake\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#i-just-want-to-use-your-trained-mos-predictor","title":"I just want to use your trained MOS predictor!","text":"<p>We utilize <code>torch.hub</code> to provide a convenient way to load pre-trained SSQA models and predict scores of wav files or torch tensors. </p> <p>Note</p> <p>You don't need to install sheet following the installation instructions. However, you might need to install the following:</p> <ul> <li>torch</li> <li>h5py</li> <li>s3prl</li> </ul> <pre><code># load pre-trained model\n&gt;&gt;&gt; predictor = torch.hub.load(\"unilight/sheet:v0.1.0\", \"default\", trust_repo=True, force_reload=True)\n# if you want to use cuda\n&gt;&gt;&gt; predictor.model.cuda()\n\n# you can either provide a path to your wav file\n&gt;&gt;&gt; predictor.predict(wav_path=\"/path/to/wav/file.wav\")\n3.6066928\n\n# or provide a torch tensor with shape [num_samples]\n&gt;&gt;&gt; predictor.predict(wav=torch.rand(16000))\n1.5806346\n# if you put the model on cuda...\n&gt;&gt;&gt; predictor.predict(wav=torch.rand(16000).cuda())\n1.5806346\n</code></pre>"},{"location":"#i-am-new-to-mos-prediction-research-i-want-to-train-models","title":"I am new to MOS prediction research. I want to train models!","text":"<p>You are in the right place! This is the main purpose of SHEET. We provide complete experiment recipes, i.e., set of scripts to download and process the dataset, train and evaluate models. </p> <p>Please follow the installation instructions first, then see training guide for how to start.</p>"},{"location":"#i-already-have-my-mos-predictor-i-just-want-to-do-benchmarking","title":"I already have my MOS predictor. I just want to do benchmarking!","text":"<p>We provide scripts to collect the test sets conveniently. These scripts can be run on Linux-like platforms with basic python requirements, such that you do not need to instal all the heavy packages, like PyTorch.</p> <p>Please see benchmarking guide for detailed instructions.</p>"},{"location":"#supported-models","title":"Supported models","text":""},{"location":"#ldnet","title":"LDNet","text":"<ul> <li>Original repo link: https://github.com/unilight/LDNet</li> <li>Paper link: https://arxiv.org/abs/2110.09103</li> <li>Example config: egs/bvcc/conf/ldnet-ml.yaml</li> </ul>"},{"location":"#ssl-mos","title":"SSL-MOS","text":"<ul> <li>Original repo link: https://github.com/nii-yamagishilab/mos-finetune-ssl</li> <li>Paper link: https://arxiv.org/abs/2110.02635</li> <li>Example config: egs/bvcc/conf/ssl-mos-wav2vec2.yaml</li> <li>Notes: We made some modifications to the original implementation. Please see the paper for more details.</li> </ul>"},{"location":"#utmos-strong-learner","title":"UTMOS (Strong learner)","text":"<ul> <li>Original repo link: https://github.com/sarulab-speech/UTMOS22/tree/master/strong</li> <li>Paper link: https://arxiv.org/abs/2204.02152</li> <li>Example config: egs/bvcc/conf/utmos-strong.yaml</li> </ul> <p>Note</p> <p>After discussion with the first author of UTMOS, Takaaki, we feel that UTMOS = SSL-MOS + listener modeling + contrastive loss + several model arch and training differences. Takaaki also felt that using phoneme and reference is not really helpful for UTMOS strong alone. Therefore we did not implement every component of UTMOS strong. For instance, we did not use domain ID and data augmentation.</p>"},{"location":"#alignnet","title":"AlignNet","text":"<ul> <li>Original repo link: https://github.com/NTIA/alignnet</li> <li>Paper link: https://arxiv.org/abs/22406.10205</li> <li>Example config: egs/bvcc+nisqa+pstn+singmos+somos+tencent+tmhint-qi/conf/alignnet-wav2vec2.yaml</li> </ul>"},{"location":"#supported-features","title":"Supported features","text":""},{"location":"#modeling","title":"Modeling","text":"<ul> <li>Listener modeling</li> <li>Self-supervised learning (SSL) based encoder, supported by S3PRL</li> </ul> <p>Note</p> <p>Find the complete list of supported SSL models here</p>"},{"location":"#training","title":"Training","text":"<ul> <li>Automatic best-n model saving and early stopiing based on given validation criterion</li> <li>Visualization, including predicted score distribution, scatter plot of utterance and system level scores</li> <li>Model averaging</li> <li>Model ensembling by stacking</li> </ul>"},{"location":"benchmarking/","title":"Benchmarking guide","text":"<p>If you don't want to train SSQA models but just want to get the test sets, you are in the right place. The design is that you don't need to run the installation, which can be time-consuming.</p>"},{"location":"benchmarking/#data-preparation","title":"Data preparation","text":"<p>First, please set the following two variables in <code>egs/BENCHMARKS/get_all_bencmarks_installation_free.sh</code>: - <code>db_root</code>: the root directory to save the raw datsets, including the wav files. - <code>datadir</code>: the directory to save the <code>csv</code> files.</p> <p>Then, executing the commands below.</p> <pre><code>cd egs/BENCHMARKS\n\n# data download.\n# for certain datasets (ex., BVCC or BC19), folow the instructions to finish downloading.\n./utils/BENCHMARKS/get_all_bencmarks_installation_free.sh --stage -1 --stop_stage -1\n\n# data preparation. this step generates the csv files.\n./utils/BENCHMARKS/get_all_bencmarks_installation_free.sh --stage 0 --stop_stage 0\n</code></pre> <p>After the data preparation stage is done, you should get the .csv files in <code>datadir</code>. Each csv file contains the following columns:</p> <ul> <li><code>wav_path</code>: Absolute paths to the wav files.</li> <li><code>system_id</code>: System ID. This is usually for synthetic datasets. For some datasets, this is set to a dump ID.</li> <li><code>sample_id</code>: Sample ID. Should be unique within one single dataset.</li> <li><code>avg_score</code>: The ground truth MOS of the sample.</li> </ul>"},{"location":"benchmarking/#metric-calculation","title":"Metric calculation","text":"<p>You may then use your MOS predictor to run inference over the samples in the test set csv files. We suggest you to overwrite (or resave) the csv file by adding a <code>answer</code> column. Then, you can use the following command to calculate the metrics. Here we provide an example results csv file in assets/example_results.csv, which is based on BVCC test.</p> <pre><code>python utils/calculate_metrics.py --csv assets/example_results.csv\n</code></pre> <p>The result will be:</p> <pre><code>[UTT][ MSE = 0.271 | LCC = 0.867 | SRCC = 0.870 | KTAU = 0.693 ] [SYS][ MSE = 0.123 | LCC = 0.9299 | SRCC = 0.9302  | KTAU = 0.777 ]\n</code></pre>"},{"location":"mos-bench/","title":"MOS-Bench","text":"<p>Last updated: September 2025</p> <p>We provide a (possibly) easier-to-read Google Spreadsheet file.</p>"},{"location":"mos-bench/#training-sets","title":"Training sets","text":"Name Type Language FS (kHz) # samples (train/dev) BVCC TTS, VC, natural speech English 16 4944/1066 SOMOS TTS, natural speech English 24 14100/3000 SingMOS SVS, SVC, natural singing voice Chinese, Japanese 16 2000/544 NISQA artificial &amp; real distorted speech, clean speech English 48 11020/2700 TMHINT-QI artificial noisy speech, enhanced speech, clean speech Chinese 16 11644/1293 Tencent artificial distorted speech, clean speech Chinese 16 10408/1155 PSTN PSTN speech, artificial distorted speech English 8 52839/5870 URGENT2024-MOS artificial &amp; real distorted speech, enhanced speech English 8-48 6210/690"},{"location":"mos-bench/#test-sets","title":"Test sets","text":"Name Type Language FS (kHz) # samples (train/dev) BVCC test (VMC22 main track) TTS, VC, natural speech English 16 1066 SOMOS test TTS, natural speech English 24 3000 BC19 (VMC22 OOD track) TTS, natural speech Chinese 16 540 BC23 Hub (VMC23 track1a) TTS, natural speech France 22 882 BC23 Spoke (VMC23 track1b) TTS, natural speech France 22 578 SVCC23 (VMC23 track2) SVC, natural singing voice English 24 4040 SingMOS test (VMC24 track2) SVS, SVC, natural singing voice Chinese, Japanese 16 645 BRSpeechMOS TTS, natural speech Brazilian-Portuguese 16 243 HablaMOS TTS, natural speech Spanish 16 408 TTSDS2 TTS English 22 4731 NISQA TEST FOR artificial distorted speech, VoIP English 48 240 NISQA TEST LIVETALK real-world distorted speech, VoIP Dutch 48 232 NISQA TEST P501 artificial distorted speech, VoIP English 48 240 TMHINT-QI test artificial noisy speech, enhanced speech, clean speech Chinese 16 1978 TMHINT-QI(S) (VMC23 track3) artificial noisy speech, enhanced speech, clean speech Chinese 16 1960 TCD-VOIP artificial distorted speech, VoIP English 48 384 VMC24 track3 artificial noisy speech, enhanced speech, clean speech English 16 280"},{"location":"training/","title":"Training guide","text":"<p>We provide complete experiment recipes, i.e., set of scripts to download and process the dataset, train and evaluate models.</p> <p>Info</p> <p>This structure originated from Kaldi, and is also used in many speech processing based repositories (ESPNet, ParallelWaveGAN, etc.).</p>"},{"location":"training/#conduct-complete-experiments-training-benchmarking","title":"Conduct complete experiments (training &amp; benchmarking)","text":"<p>You can train your own speech quality predictor using the datasets in MOS-Bench. Each The starting point of each recipe is the <code>run.sh</code> file.</p> <p>There are two ways you can train models: - Train models using a single dataset at a time. This is called \"single dataset training\". You can refer to the Training datasets in MOS-Bench section for available datasets. - Train models by pooling multiple datasets. This is called \"multiple dataset training\". Currently we support the following recipe:     - Recipe: <code>egs/bvcc+nisqa+pstn+singmos+somos+tencent+tmhint-qi</code></p> <p>After you train your models, you can run benchmarking on the test sets in MOS-Bench.</p> <p>Below, we explain the processes of training and benchmarking.</p>"},{"location":"training/#structure-of-each-training-recipe-runsh","title":"Structure of each training recipe (<code>run.sh</code>)","text":"<p>For most recipes, by default the SSL-MOS model is used. The configuration file is <code>conf/ssl-mos-wav2vec2.yaml</code>.</p>"},{"location":"training/#data-download","title":"Data download","text":"<pre><code>./run.sh --stage -1 --stop_stage -1\n</code></pre> <p>By default, the dataset will be put in the <code>downloads</code> folder in each recipe folder. However, you can change the destination to your favor.</p> <p>For most datasets, this step automatically calls the <code>local/data_download.sh</code> script to fetch the dataset. However, some datasets (like <code>bvcc</code>) does not come with an one-step automatic download script due to the data policy. In that case, please follow the instructions.</p>"},{"location":"training/#data-preparation","title":"Data preparation","text":"<pre><code>./run.sh --stage 0 --stop_stage 0\n</code></pre> <p>This step processes the downloaded dataset and generate <code>.csv</code> files for the training and development (and possibly testing) sets. The generated data files are in <code>data</code>. For instance, <code>bvcc/data/bvcc_&lt;train, dev, test&gt;.csv</code> files will be generated.</p> <p>The common data format across all recipes is csv. Each csv file always contains the following columns:</p> <ul> <li><code>wav_path</code>: Absolute paths to the wav files.</li> <li><code>score</code>: This is the listener-dependent score by default. If not available, then it will be the MOS.</li> <li><code>system_id</code>: System ID. This is usually for synthetic datasets. For some datasets, this is set to a dump ID.</li> <li><code>sample_id</code>: Sample ID. Should be unique within one single dataset.</li> </ul> <p>Optionally, the following columns may exist:</p> <ul> <li><code>listener_id</code>: The original listener ID in the dataset. Only when listener ID is available in the dataset.</li> <li><code>listener_idx</code> : A listener INDEX for the listener ID. This is an integer scalar.</li> <li><code>phoneme</code>: This only exists for BVCC. Not really important.</li> <li><code>cluster</code>: This only exists for BVCC. Not really important.</li> <li><code>reference</code>: This only exists for BVCC. Not really important.</li> </ul> <p>Note</p> <p>Stage 1 is reserved for pre-trained model download usage Please see Download pre-trained SSQA models for details..</p>"},{"location":"training/#training","title":"Training","text":"<pre><code>./run.sh --stage 2 --stop_stage 2 \\\n    --conf &lt;conf/config.yml&gt; --tag &lt;tag&gt; --seed &lt;seed&gt; # these are optionals\n</code></pre> <p>Training is launched within this stage. All generated artifacts will be saved to a <code>expdir</code>, which is by default <code>exp/ssl-mos-wav2vec2-1337</code>. The <code>1337</code> is the random seed set by <code>--seed</code>. If <code>--tag</code> is specified, then they will be saved in <code>exp/&lt;tag&gt;-1337</code>. The model checkpoints can be found in the expdir.  Also, you can check the <code>exp/ssl-mos-wav2vec2-1337/intermediate_results</code> to see some plots to monitor the training process.</p>"},{"location":"training/#structure-of-each-benchmark-recipe-benchmarksrun_xxx_testsh","title":"Structure of each benchmark recipe (<code>BENCHMARKS/run_XXX_test.sh</code>)","text":"<p>After the training process described above, INSIDE THOSE RECIPES you can follow the commands below to do benchmarking.</p> <p>As an example, we assume we trained a model in <code>egs/bvcc</code>, and we want to run inference with the BVCC test set. That is, we will be executing <code>run_bvcc_test.sh</code>.</p>"},{"location":"training/#data-download-preparation","title":"Data download &amp; preparation","text":"<pre><code># data download.\n# for certain datasets (ex., BVCC or BC19), folow the instructions to finish downloading.\n./utils/BENCHMARKS/run_bvcc_test.sh --stage -1 --stop_stage -1\n\n# data preparation\n./utils/BENCHMARKS/run_bvcc_test.sh --stage 0 --stop_stage 0\n</code></pre> <p>The purpose of this stage is the same as the data download &amp; preparation stage as described in the training process. The generated data csv files will be stored in the <code>data</code> folder IN EACH CORRESPONDING TEST SET RECIPE FOLDER. For instance, for <code>run_bvcc_test.sh</code>, the generated data csv files will be in <code>bvcc/data/bvcc_&lt;dev, test&gt;.sh</code>. This also holds for test sets without training recipes: for instance, <code>run_vmc23_test.sh</code> will generate data csv files to <code>vmc23/data/vmc23_track&lt;1a, 1b, 2, 3&gt;_test</code>.</p>"},{"location":"training/#inference","title":"Inference","text":"<p>The following command runs parametric inference.</p> <pre><code>./utils/BENCHMARKS/run_bvcc_test.sh --stage 1 --stop_stage 1 \\\\\n    --conf &lt;conf/config.yml&gt; --checkpoint &lt;checkpoint file&gt; --tag &lt;tag&gt; # these are optionals\n</code></pre> <p>Inference is done within this stage. The results will be saved in <code>exp/&lt;expdir&gt;/results/&lt;checkpoint_name&gt;/&lt;test_set_name&gt;/</code>. For instance, <code>exp/ssl-mos-wav2vec2-1337/results/checkpoint-best/bvcc_test</code>. Inside you can find the following files: - <code>inference.log</code>: log file of the inference script, along with the calculated metrics. - <code>distribution.png</code>: distribution over the score range (1-5). - <code>utt_scatter_plot.png</code>: utterance-wise scatter plot of the ground truth and the predicted scores. - <code>sys_scatter_plot.png</code>: system-wise scatter plot of the ground truth and the predicted scores.</p> <p>By default, the <code>checkpoint-best.pkl</code> is used, which is a symbolic link that points to the best performing model checkpoint (depending on the <code>best_model_criterion</code> field in the config file.) You can specify a different checkpoint file with <code>--checkpoint</code>.</p>"},{"location":"training/#non-parametric-inference","title":"Non-parametric inference","text":"<p>You can also run non-parametric inference for certain models. However, not all recipes and models support it. - Note: as of 2024.10, currently, only the <code>egs/bvcc+nisqa+pstn+singmos+somos+tencent+tmhint-qi</code> recipe supports non-parametric inference. However, it is not difficult to add it to other recipes. It's just that we haven't testes it yet.</p> <p>If you want to run non-parametric inference, you need to prepare the datastore first. This can be done by:</p> <pre><code>./run.sh --stage 3 --stop_stage 3 \\\n    --conf &lt;conf/config.yml&gt; --tag &lt;tag&gt; --seed &lt;seed&gt; # these are optionals\n</code></pre> <p>Then, you can do non-parametric inference with the following command:</p> <pre><code>./utils/BENCHMARKS/run_bvcc_test.sh --stage 3 --stop_stage 3 \\\n    --np_inference_mode &lt;naive_knn/domain_id_knn_1&gt; \\\n    --conf &lt;conf/config.yml&gt; --checkpoint --tag &lt;tag&gt; # these are optionals\n</code></pre> <p>Note that the results will be then stored in <code>exp/&lt;expdir&gt;/results/np_&lt;checkpoint_name&gt;/&lt;np_inference_mode&gt;/&lt;test_set_name&gt;/</code>. For instance, <code>egs/bvcc+nisqa+pstn+singmos+somos+tencent+tmhint-qi/exp/alignnet-wav2vec2-2337/results/np_checkpoint-best/naive_knn/bc19_test</code>.</p>"},{"location":"training/#run-all-benchmarks-at-once","title":"Run all benchmarks at once","text":"<p>You can also run all benchmarks at once. First, download and prepare all the benchmark sets.</p> <pre><code># data download.\n# for certain datasets (ex., BVCC or BC19), folow the instructions to finish downloading.\n./utils/BENCHMARKS/get_all_bencmarks.sh --stage -1 --stop_stage -1\n\n# data preparation\n./utils/BENCHMARKS/get_all_bencmarks.sh --stage 0 --stop_stage 0\n</code></pre> <p>Then, run inference based on the mode: - Parametric inference:   <pre><code>./utils/BENCHMARKS/run_all_bencmarks.sh \\\n  --conf conf/ssl-mos-wav2vec2 --checkpoint &lt;checkpoint&gt; --tag &lt;tag&gt; --seed &lt;seed&gt;  # these are optionals\n</code></pre> - Non-parametric inference:</p> <pre><code>./utils/BENCHMARKS/run_all_bencmarks.sh --np_inference_mode &lt;naive_knn/domain_id_knn_1&gt; \\\n  --conf conf/ssl-mos-wav2vec2 --checkpoint &lt;checkpoint&gt; --tag &lt;tag&gt; --seed &lt;seed&gt;  # these are optionals\n</code></pre>"},{"location":"training/#download-pre-trained-ssqa-models-to-reproduce-the-paper-results","title":"Download pre-trained SSQA models to reproduce the paper results","text":"<p>We provide pre-trained model checkpoints to reproduce the results in our paper. They are hosted on , and you can see all the supported models in the model repo.</p> <p>The pre-trained models can be downloaded by executing stage 1 in each recipe.</p> <pre><code>./run.sh --stage 1 --stop_stage 1\n</code></pre> <p>The downloaded models will be in stored in <code>exp/pt_&lt;model_tag&gt;_&lt;seed&gt;</code>. For example, <code>exp/pt_ssl-mos-wav2vec2-2337</code>. Then, you can follow the instructions here to run inference on all benchmarks.</p>"}]}